## `llama.cpp`

`.gguf`文件是经过量化、优化的模型文件，包含了所有的神经网络参数（权重、偏置项），并且通过高效的二进制存储格式，可以快速加载到内存中，并直接在本地硬件上进行推理。

### 模型量化

一般传统的模型训练中，权重和偏置项都是32位的浮点数，通过量化可能转换成8位整数，这个精细的数值会被简化成较小的整数，会导致一定的推理误差，但是这种误差是微小的，特别是在自然语言处理中，模型本身的鲁棒性可以弥补一些细微的误差。

**优化：**

量化感知训练允许在量化后对模型进行微调，帮助模型在低精度下依然保持较好的性能。

对每个层每个参数进行不同程度的量化，甚至采用混合精度，能够减小精度损失。

### llama运行原理

运行原理就是在输入的gguf文件进行解析，这个文件内部嵌入了训练的参数、偏置项、编码规则，词表以及模型结构的信息，以供llama做解析。那llama就需要解析其中提供的信息，初始化推理引擎，这个过程还需要做量化和加速，然后模型推理完，输出返回结果。

```objectivec
            开始
              │
              ▼
            解析命令行参数（获取模型路径）
              │
              ▼
            加载模型文件（GGUF 格式）
              │
              ▼
            检查硬件（CPU 或 GPU）
              │
              ▼
            如果是 CPU：
              │
              ├── 使用 AVX/AVX2 优化
              └── 使用 OpenMP 并行化计算
              │
              ▼
            如果是 GPU：
              │
              ├── 使用 CUDA（NVIDIA GPU）
              └── 使用 OpenCL（AMD GPU）
              │
              ▼
            执行前向传播（矩阵计算）
              │
              ▼
            输出结果
              │
              ▼
            结束
```

### llama的功能

1. 从Hugging Face下载支持的模型，可能下载格式为`.pth/safetensors`

```bash
python3 convert.py --input_model model.pth --output_model model.gguf
```

使用官方提供的脚本将模型转换成`GGUF`格式。

1. 为了减少内存占用并提高推理速度，你可以对模型进行量化：

```bash
./quantize model.gguf model-quantized.gguf q4_0
```

### Transformer架构的多头注意力机制推理流程

```shell
输入 tokens: ["今天", "的", "天气"]
        ↓
+-------------------------------+            +-------------------------------+
|      Token Embeddings         |            |      Token Embeddings         |
|      (Q, K, V for each token) |            |      (Q, K, V for each token) |
+-------------------------------+            +-------------------------------+
       ↓                                     ↓
       ↓                                     ↓
  +------------+       +------------+    +------------+       +------------+
  |  Head 1:   |       |  Head 2:   |    |  Head 3:   |       |  Head 4:   |
  | Attention  |       | Attention  |    | Attention  |       | Attention  |
  | (Q, K, V)  |       | (Q, K, V)  |    | (Q, K, V)  |       | (Q, K, V)  |
  +------------+       +------------+    +------------+       +------------+
        ↓                     ↓                 ↓                     ↓
        ↓                     ↓                 ↓                     ↓
+-------------------------------------+   +-------------------------------------+
|    Attention Scores (Q · K^T)       |   |    Attention Scores (Q · K^T)       |
|    (Dot product for each token)     |   |    (Dot product for each token)     |
+-------------------------------------+   +-------------------------------------+
         ↓                                    ↓
         ↓                                    ↓ //这一层可能会除一个常数防止梯度爆炸
  +-------------------+           +-------------------+
  | Softmax (Weights) |           | Softmax (Weights) |
  +-------------------+           +-------------------+
         ↓                                    ↓
         ↓                                    ↓
   +------------------+            +------------------+
   | Weighted Sum of  |            | Weighted Sum of  |
   | Values (V)       |            | Values (V)       |
   +------------------+            +------------------+
         ↓                                    ↓
         ↓                                    ↓
      +-----------------------------------------------+
      | Concatenate all heads' output (Head 1 + Head 2 + ...) |
      +-----------------------------------------------+
                             ↓
                     +-------------------+
                     | Final Linear Layer |
                     +-------------------+
                             ↓
                     +-------------------+
                     | Output (final token representations) |
                     +-------------------+
```

多头注意力机制的好处在于多个头可能关注不同的语义，有的头可能关注语法结构，有的头关注情感倾向，有的头关注主谓宾，每个头做注意力，最后拼接起来，再做一次线性变换。

### 线性变换的意义

1. 首先是需要进行**维度的对其**，对其维度才能够为后面输入（残差网络和前馈神经网络）做铺垫是一个投影操作。
2. 学习**不同头之间的组合方式**，多个头学到的信息可能互补或者冗余，简单拼接只是”并排放在一起“，而线性变换可以让模型学会如何融合这些头的输出。
3. **增强表达能力，**线性层的参数![img](G:\CPP\CS_review\assets\6d6d7061b6d764362fc434f6e45d7fff.svg+xml)是可以学习的，相当于让模型自己决定那些头的组合是有用的。

### Transformer的架构优势是什么？

每一个词都会通过嵌入层，变成一个向量（嵌入层+位置编码），所有次的向量同时进入多头注意力层，每个词的表示都可以直接和其他所有词”交流“（通过注意力），所有操作都是基于矩阵运算。这意味着所有词都可以同时计算，适合GPU的并行处理，批处理效率高，训练速度快。

### 残差网络是做什么的

主要是解决神经网络在训练过程中可能遇到的**梯度消失/爆炸问题**。随着网络深度的增加，反向传播是梯度可能会逐渐消失，导致无法有效训练非常深的网络。残差网络的引入就是能够使得网络能够更容易学习到深层次特征，核心思想就是引入**跳跃连接，**也就是将输入加到输出上，避免信息的丢失。

**LayerNorm** 通过对每个样本的特征进行标准化，改善了训练的稳定性，并加速了收敛。其缩放参数 γ 和偏移参数 β是通过训练得到的。  